{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d242a721-a028-42fc-abd0-bb4ad8bfaea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.common import exceptions\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import PrettyPrinter\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bd969b9-8f4e-42bf-8bc1-2d0e3434b1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc66641-2063-44d6-9feb-763cfcbb7576",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv('GIIRScraper_04_14_2022__12_15_27.csv')\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a405c0a-a399-44f0-b8e3-30299527a75f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84e081fc-313e-4df0-8044-575c244d46f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_input():\n",
    "    while True:\n",
    "        print('Enter dates below | Enter q to quit')\n",
    "        low = input('\\tLow e.g. January 1, 2022: ')\n",
    "        high = input('\\tHigh e.g. January 1, 2022: ')\n",
    "        if high == '':\n",
    "            high = None\n",
    "        if low == '':\n",
    "            low == None  \n",
    "        if low == 'q' or high == 'q':\n",
    "            break\n",
    "            sys.exit()\n",
    "        try:\n",
    "            low = pd.to_datetime(low) \n",
    "            high = pd.to_datetime(high)\n",
    "            break\n",
    "        except:\n",
    "            print('Enter a correct date format')\n",
    "            continue\n",
    "            \n",
    "    return low, high\n",
    "    \n",
    "\n",
    "def scrape_data(lower=None, upper=None):\n",
    "    # SCRAPE DATA\n",
    "    browser = webdriver.Chrome()\n",
    "    browser.get('https://www.giiresearch.com/material_report.shtml')\n",
    "\n",
    "    # data columns\n",
    "    data = dict()\n",
    "    data['Published_Date'] = []\n",
    "    data['Category'] = []\n",
    "    data['Report_Title'] = []\n",
    "    data['Summary'] = []\n",
    "    data['No_of_Pages'] = []\n",
    "    data['Table_of_Contents'] = []\n",
    "    data['List_of_Tables'] = []\n",
    "\n",
    "\n",
    "    # get link to each record on search page\n",
    "    links = []\n",
    "    tables = browser.find_elements(by=By.CLASS_NAME, value='plist_item')\n",
    "\n",
    "    for table in tables:\n",
    "        links.append(table\n",
    "                     .find_element(By.CLASS_NAME, 'plist_title')\n",
    "                     .find_element(By.CLASS_NAME, 'plist_t_box')\n",
    "                     .find_element(By.CLASS_NAME, 'list_title')\n",
    "                     .find_element(By.TAG_NAME, 'a')\n",
    "                     .get_attribute('href')\n",
    "                    )\n",
    "\n",
    "    # get record data from each page\n",
    "    for link in links:\n",
    "        browser.get(link)\n",
    "         # to handle unfound pages\n",
    "        try:\n",
    "            not_found = (browser.find_element(By.CSS_SELECTOR, '#Body_Wide > table > tbody > tr > td > h1')\n",
    "                         .find_element(By.TAG_NAME, 'span').text\n",
    "                        )\n",
    "            data['Published_Date'].append(np.NaN)\n",
    "            data['Report_Title'].append(link)\n",
    "            data['Category'].append(not_found)\n",
    "            data['Summary'].append(not_found)\n",
    "            data['No_of_Pages'].append(not_found)\n",
    "            data['Table_of_Contents'].append(not_found)\n",
    "            data['List_of_Tables'].append(not_found)\n",
    "            continue\n",
    "        except exceptions.NoSuchElementException:\n",
    "            # date\n",
    "            date = (browser\n",
    "                    .find_element(By.CSS_SELECTOR, '#Content_Body > div.prodinfo_body > div.prod_info_box > nobr:nth-child(1) > span > time')\n",
    "                    .text)\n",
    "            data['Published_Date'].append(date)\n",
    "\n",
    "            # title\n",
    "            title = (browser\n",
    "                     .find_element(By.CSS_SELECTOR, '#Content_Body > div.prodinfo_body > table > tbody > tr > td.prdinfo_title > h1 > span')\n",
    "                     .text\n",
    "            )\n",
    "            data['Report_Title'].append(title)\n",
    "\n",
    "            # industry\n",
    "            industry = browser.find_element(By.CSS_SELECTOR, '#Body_Bread > div > a:nth-child(3)').text\n",
    "            data['Category'].append(industry)\n",
    "\n",
    "            # summary \n",
    "            summary = browser.find_element(By.CSS_SELECTOR, '#INTRODUCTION > div.cntSecContent').text\n",
    "            data['Summary'].append(summary)\n",
    "\n",
    "            # No of pages\n",
    "            try:\n",
    "                p_nos =(int(browser.find_element(By.CSS_SELECTOR, '#Content_Body > div.prodinfo_body > div.prod_info_box > nobr:nth-child(5) > span')\n",
    "                        .text.split(' ')[0]))\n",
    "            except ValueError:\n",
    "                p_nos = np.NaN\n",
    "            except exceptions.NoSuchElementException:\n",
    "                p_nos = np.NaN\n",
    "            data['No_of_Pages'].append(p_nos)\n",
    "            # Table of contents\n",
    "            try:\n",
    "                browser.find_element(By.ID, 'Tab').find_elements(By.TAG_NAME, 'li')[1].click()\n",
    "                t_o_c = browser.find_element(By.ID, 'TOC').text\n",
    "            except:\n",
    "                t_o_c=np.NaN\n",
    "            data['Table_of_Contents'].append(t_o_c)\n",
    "\n",
    "            # List of Tables\n",
    "            try:\n",
    "                browser.find_element(By.ID, 'Tab').find_elements(By.TAG_NAME, 'li')[2].click()\n",
    "                l_o_t = browser.find_element(By.ID, 'LOT').text\n",
    "            except:\n",
    "                l_o_t=np.NaN\n",
    "            data['List_of_Tables'].append(l_o_t)\n",
    "    browser.close()\n",
    "\n",
    "    # CONVERT TO DATAFRAME\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv('GIIR_records_unfiltered.csv', index=False)\n",
    "    df.Published_Date = pd.to_datetime(df.Published_Date) # convert date to date time\n",
    "\n",
    "    # FILTER TABLE BASED ON GIVEN DATE\n",
    "    if high and low: # check for date range arguements\n",
    "        df = df[df.Published_Date[(df.Published_Date >= low)] <= high] # filter \n",
    "    elif low: \n",
    "        df = df[(df.Published_Date >= low)] # filter\n",
    "\n",
    "    # convert date obj to str again\n",
    "    df.Published_Date = df.Published_Date.dt.strftime('%B %d %Y')\n",
    "    # SAVE TABLE AS CSV FILE\n",
    "    df.to_csv('GIIR_records_filtered.csv', index=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ad0cd5cd-eede-4af4-8e50-3d5bfb2e8683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter dates below | Enter q to quit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\tLow e.g. January 1, 2022:  April 10, 2022\n",
      "\tHigh e.g. January 1, 2022:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIIR csv file saved!\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ikwang/.pyenv/versions/3.8.6/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3452: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if '__main__' == __name__:\n",
    "    low, high = get_input()\n",
    "    scrape_data(low, high)\n",
    "    print('GIIR csv file saved!')\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea7545e-0fe8-4c54-9765-deaeb770f2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b4306c-eded-4c7d-9f16-2bf7fce06105",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048b5e6d-7771-4910-9562-1a74e19789bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3.8.6 (main env)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
